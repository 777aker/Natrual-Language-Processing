{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CSCI 3832, Spring 2020, Lecture 34 â€” HW 5 examples & getting started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "from scipy.special import softmax\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 1: building a Mini-HMM\n",
    "========================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreedyHMM:\n",
    "  \n",
    "    def __init__(self):\n",
    "        # here are the items that we'll need to gather\n",
    "        self.pi = Counter() # pi\n",
    "        self.transitions = {} # this is A\n",
    "        self.emissions = {} # this is B\n",
    "        self.states = set() # Q\n",
    "        self.vocab = set() # O\n",
    "\n",
    "    def train(self, example):\n",
    "        \"\"\"\n",
    "        Trains this model based on the given input data\n",
    "        params: examples - a list of (token, label) tuples\n",
    "        return: None\n",
    "        This example is going to be list of token, label, in\n",
    "        hw5 it is a list of lists of examples, this is one example\n",
    "        We won't be implementing laplace smoothing which is something you\n",
    "        need to do in hw5\n",
    "        \"\"\"\n",
    "        \n",
    "        # pi (initial probability) * p(w_i | t_i)\n",
    "        \n",
    "        # prev_prob * p(w_i | t_i) * p(t_i | t_i-1)\n",
    "        \n",
    "        # in hw5 there will be another loop through the sentences\n",
    "        for i in range(len(example)):\n",
    "            # tuple of word and state so need 0 word and 1 state\n",
    "            word = example[i][0]\n",
    "            state = example[i][1]\n",
    "            if i == 0:\n",
    "                # count(sentences that start with state) / count of sentences\n",
    "                # the fraction of sentences that start with our state\n",
    "                # how many sentences start with our state\n",
    "                self.pi[state] += 1\n",
    "            else:\n",
    "                # p(t_i | t_(i-1))\n",
    "                # count(t_i - 1, t_i) / count(t_i - 1)\n",
    "                # transitions[prev_state][state] = count\n",
    "                # sum(transitions[prev_state].values()) = count(t_i - 1)\n",
    "                if example[i - 1][1] not in self.transitions:\n",
    "                    self.transitions[example[i - 1][1]] = Counter()\n",
    "                # nested dictionary, nicer, could do seperate dictionaries\n",
    "                self.transitions[example[i - 1][1]][state] += 1\n",
    "                \n",
    "                #TODO: think about what happens for the final state in the sentence\n",
    "        \n",
    "            # p(w_i | t_i)\n",
    "            # count(t_i, w_i) / count(t_i)\n",
    "            # emissions[prev_state][word] = count(t_i, w_i)\n",
    "            # now we can sum are emissions just like we did with transitions\n",
    "            # sum(emissions[state].values()) = count(t_i)\n",
    "            if state not in self.emissions:\n",
    "                self.emissions[state] = Counter()\n",
    "            self.emissions[state][word] += 1\n",
    "            \n",
    "        \n",
    "        #TODO: make into percentages \n",
    "        print(self.pi)\n",
    "        \n",
    "        print(self.transitions)\n",
    "        print(self.emissions)\n",
    "        pass\n",
    "    \n",
    "    def greedy_decode(self, data):\n",
    "        \"\"\"\n",
    "        params: data - a list of tokens\n",
    "        return: a list of [(token, label)...] tuples\n",
    "        We will be doing viterbi with beam width of 1\n",
    "        \"\"\"\n",
    "        \n",
    "        # hw5 we will run viterbi\n",
    "        # here we are going to implement it with a beam size of 1\n",
    "        prev_prob = 0\n",
    "        prev_state = None\n",
    "        for i in range(len(data)):\n",
    "            word = data[i]\n",
    "            max_prob = 0\n",
    "            max_state = None\n",
    "            # could save all the states in a seperate variable if you wanted\n",
    "            for state in self.emissions.keys():\n",
    "                emissions = self.emissions[state][word] / sum(self.emissions[state].values())\n",
    "                if i == 0:\n",
    "                    # pi * emission prob\n",
    "                    pi_val = self.pi[state] / sum(self.pi.values())\n",
    "                    prob = emission * pi_val\n",
    "                    # probabilities we are beginning in each state\n",
    "                else:\n",
    "                    transition = self.transitions[prev_state][state] / sum(self.transitinons[prev_state].values())\n",
    "                    prob = prev_prob * transition * emission\n",
    "                # could also do a list and argmax\n",
    "                if max_state in None or prob > max_prob:\n",
    "                    max_state = state\n",
    "                    max_prob = prob\n",
    "            \n",
    "            prev_prob = max_prob\n",
    "            prev_state = max_state\n",
    "            print(word)\n",
    "            print(max_prob)\n",
    "            print(max_state)\n",
    "            # if you wanted this as a list you could concatenate the results and return an entire list\n",
    "        \n",
    "        pass\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_data = [(\"I\", \"NOUN\"), (\"went\", \"OTHER\"), (\"to\", \"OTHER\"), (\"the\", \"OTHER\"), (\"zoo\", \"NOUN\"), (\"and\", \"OTHER\"), \\\n",
    "               (\"I\", \"NOUN\"), (\"saw\", \"OTHER\"), (\"a\", \"OTHER\"), (\"penguin\", \"NOUN\"), (\"and\", \"OTHER\"), (\"sparkly\", \"OTHER\"), (\"jaguars\", \"NOUN\"), (\"!\", \"OTHER\")]\n",
    "test_data = \"I went to the penguin park .\".split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'NOUN': 1})\n",
      "{'NOUN': Counter({'OTHER': 5}), 'OTHER': Counter({'OTHER': 4, 'NOUN': 4})}\n",
      "{'NOUN': Counter({'I': 2, 'zoo': 1, 'penguin': 1, 'jaguars': 1}), 'OTHER': Counter({'and': 2, 'went': 1, 'to': 1, 'the': 1, 'saw': 1, 'a': 1, 'sparkly': 1, '!': 1})}\n"
     ]
    }
   ],
   "source": [
    "hmm = GreedyHMM()\n",
    "hmm.train(train_data)\n",
    "hmm.greedy_decode(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differences between this HMM and the one that you're implementing for HW 5?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to handle multiple examples\n",
    "\n",
    "Need to laplace smooth\n",
    "\n",
    "Do all of viterbi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 2: building a Mini-MEMM\n",
    "========================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreedyMEMM:\n",
    "    # this can get very slow very quickly\n",
    "  \n",
    "    def __init__(self):\n",
    "        # TODO: randomly initialize your weights\n",
    "        self.weights = np.array([[1, 0, 1],[2, -3, -0.5],[0, 0.5, -1]])\n",
    "        #self.weights = 2 * np.random.random_sample((len(self.categories), self.num_feats + 1)) - 1\n",
    "        self.num_feats = 2\n",
    "        self.states = [1, 2, 3]\n",
    "\n",
    "    def train(self, examples, iterations = 100, learning_rate = 0.1):\n",
    "        \"\"\"\n",
    "        Trains this model based on the given input data\n",
    "        params: examples - a list of (features, label) data, one per example\n",
    "        return: None\n",
    "        \"\"\"\n",
    "        \n",
    "        # running SGD\n",
    "        \n",
    "        # while we still need to iterate\n",
    "        # loop over our examples (in random order)\n",
    "            # get the feature representation\n",
    "            # featurized = np.array(example[0] + [1]) # add bias in\n",
    "            # calculate y_hat\n",
    "            # z = np.dot(featurized, self.weights.T)\n",
    "            # y_hat = softmax(z)\n",
    "            \n",
    "            # calculate gradients\n",
    "            # - (1 * {y - k} - p(y - k | x)) (then * x)\n",
    "            '''\n",
    "            true_label = example[1]\n",
    "            indicators = [1 if j == true_label else 0 for j in self.states]\n",
    "            gradient_factors = -(1 * indicators - y_hat)\n",
    "            gradients = gradient_factors.reshapte(gradient_factors.shape[0], 1) @ featurized.reshape(featurized.shape[0])\n",
    "            '''\n",
    "            \n",
    "            # update your weights according to the gradients multiplied by the learning rate\n",
    "            \n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def classify_single(self, data):\n",
    "        \"\"\"\n",
    "        params: data - a list of features\n",
    "        return: a list of [(token, label)...] tuples\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are our two features and then a label\n",
    "train_data = [([3, 0.75], 1), ([0, 1], 2), ([1, 1], 3), ([2, 4], 2)]\n",
    "test_data = [2, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "memm = GreedyMEMM()\n",
    "memm.train(train_data, iterations = 100)\n",
    "memm.classify_single(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differences between this MEMM and the one that you're implementing for HW 5?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General questions:\n",
    "1. what happens with unknown words?\n",
    "    1. for the HMM?\n",
    "    2. for the MEMM?\n",
    "    \n",
    "We are not going to worry about it\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HW 5 format questions:\n",
    "1. What is the `generate_probabilities` function doing?\n",
    "    1. What do the return values represent?\n",
    "2. What is the `decode` function doing?\n",
    "3. How to use the `precision`, `recall`, `f1` functions?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
