{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kelley Kelley\n",
    "## Section 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 1:\n",
    "\n",
    "a. <mark style=\"background-color: yellow !important; font-weight:bold\"> 11 </mark>\n",
    "\n",
    "<span>\n",
    "b. If you had a bigram model using Shannon's method, you could get the sentence \"< s > I am sad < UNK > < /s >\". This would be because I most oftenly follows < s >. Then I is always follow by am. After am there is a 50/50 chance of it being sad or < UNK > so I just picked sad. Then sad is always followed by an < UNK > so < UNK > is the only thing that could be generated. Finally, < UNK > is most often followed by < /s > so I picked < /s >.\n",
    "    </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2:\n",
    "$$\n",
    "P( y=spam | x ) = exp( log( P(spam) ) + \\sum_{word \\epsilon words} log( P(word | spam) ) )\n",
    "= exp( log(0.5) + log(.27) + log(.01) + log(.16) + log(.20) + log(.11) )\n",
    "= 4.752 * 10^{-6}\n",
    "$$\n",
    "$$\n",
    "P( y=not spam | x ) = exp( log( P(not spam) ) + \\sum_{word \\epsilon words} log( P(word | spam) ) )\n",
    "= exp( log(0.5) + log(.10) + log(.01) + log(.27) + log(.29) + log(.21) )\n",
    "= 8.2215 * 10^{-6}\n",
    "$$\n",
    "\n",
    "<mark style=\"background-color: yellow !important; font-weight:bold\"> P(y=spam | x) = 4.752 * 10^{-6}. Since P(not spam | x) is greater than P(spam | x), it would be labeled as \"not spam\".  </mark>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 3:\n",
    "\n",
    "$$\n",
    "P(earthquake, no earthquake, earthquake)\n",
    "= (P(10|earthquake)*0.8) * (P(30|no earthquake)P(no earthquake|earthquake)) * (P(10|earthquake)P(earthquake|no earthquake))\n",
    "= (.2*.8) * (.4*.3) * (.2*.4) = 0.001536\n",
    "$$\n",
    "\n",
    "<mark style=\"background-color: yellow !important; font-weight:bold\"> 0.001536 </mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 4:\n",
    "\n",
    "No, because sometimes a previous option that did not create the maximum probability will create a maximum in the next column. For example, if you observed 10, 10 you would get the table:\n",
    "\n",
    "| label | 10 | 10 |\n",
    "| --- | --- | --- |\n",
    "| earthquake | .8 * .2 | .16 * .7 * .2 or .1 * .4 * .2 |\n",
    "| no earthquake | .2 * .5 | .16 * .3 * .5 or .1 * .6 * .5 |\n",
    "\n",
    "| label | 10 | 10 |\n",
    "| --- | --- | --- |\n",
    "| earthquake | .16 | .0224 |\n",
    "| no earthquake | .1 | .03 |\n",
    "    \n",
    "The maximum in the final column was gotten from no earthquake, no earthquake. Which means that our final answer of no earthquake, no earthquake as the most probably, was not derived from the maximum of the first column. This disproves that Viterbi is equivalent to taking the argmax, since taking the argmax would not have led you to the same conclusion. Argmax instead would have gotten you a 0.024 for no earthquake in the final column, therefore arriving at the conclusion of earthquake, no earthquake as the most likely.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 5:\n",
    "\n",
    "Logistic Regression is considered discriminative because it purely looks for difference between things. It tryies to categorize them by figuring out what features seperate the two things. It has no idea what the things are or how to represent them or what defines them, it only knows what discriminates them from each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 6:\n",
    "\n",
    "You cannot use the same algorithm because Logistic Regression is used to assign a class to a single observation, whereas MEMM takes Logistic Regression and uses it to perform on sequences, containing Logistic Regression within itself. MEMM requires slightly different data and the ability to iterate over examples and the words within them to classify each thing, also using the prior word in many cases, and therefore differs from Logistic Regression only taking in one thing and classifying it accordingly. Therefore they can't be the same algorithm because MEMM is an algorithm that transforms Logistic Regression to be suitable for what it usually is not suitable for, and contains all of Logistic Regression within itself.\n",
    "\n",
    "MEMM psuedocode:\n",
    "Iterate over all sentences in a random order\n",
    "Iterate over all words in that sentence\n",
    "Perform Multinomial Logistic Regression for all labels on the current word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 7:\n",
    "\n",
    "Yes. In an HMM we seperate P( state | observation) into P( observation | state) * P(state | state-1) and we directly model state-1. The MEMM just directly computes P(state | observation) by training it on P(state | observation, state-1), so it simply combines the transition probabilities in with the observations to get our current state instead of modeling it seperately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 8:\n",
    "\n",
    "a. \n",
    "\n",
    "uses an exaclamation point\n",
    "\n",
    "counts of positive words from the list [joy, brighter, bright, funny, real, joyous, good]\n",
    "\n",
    "log of the length of the review\n",
    "\n",
    "counts of negative words from the list [loathing, mediocrity, mediocre, terrible, unpleasant, mean, sadistic]\n",
    "\n",
    "b.\n",
    "\n",
    "sentence1: [0, 1, log(30), 0, 1]\n",
    "\n",
    "sentence2: [0, 0, log(18), 3, 1]\n",
    "\n",
    "c.\n",
    "\n",
    "$$\n",
    "[0, 1, log(30), 0, 1]^T[0.5, 1, -0.5, -1] = 0.511\n",
    "$$\n",
    "$$\n",
    "P(pos) = \\frac{1}{1+e^{-0.511}} = .625\n",
    "$$\n",
    "$$\n",
    "P(neg) = 1 - .625 = .375\n",
    "$$\n",
    "sentence1 is labeled positive\n",
    "$$\n",
    "[0, 0, log(18), 3, 1]^T[0.5, 1, -0.5, -1, 0.25] = -3.378\n",
    "$$\n",
    "$$\n",
    "P(pos) = \\frac{1}{1+e^{-(-3.378)}} = 0.033\n",
    "$$\n",
    "$$\n",
    "P(neg) = 1 - 0.033 = 0.967\n",
    "$$\n",
    "sentence2 is labeled negative\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 9:\n",
    "\n",
    "<mark style=\"background-color: yellow !important; font-weight:bold\"> a. b. d. </mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 10:\n",
    "\n",
    "<mark style=\"background-color: yellow !important; font-weight:bold\"> B. </mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 11:\n",
    "\n",
    "<mark style=\"background-color: yellow !important; font-weight:bold\"> a. b. </mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 12:\n",
    "\n",
    "<mark style=\"background-color: yellow !important; font-weight:bold\"> maximize, minimize </mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 13:\n",
    "\n",
    "<mark style=\"background-color: yellow !important; font-weight:bold\"> c. </mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 14:\n",
    "\n",
    "<mark style=\"background-color: yellow !important; font-weight:bold\"> a. </mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 15:\n",
    "\n",
    "<mark style=\"background-color: yellow !important; font-weight:bold\"> b. </mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Citations:\n",
    "\n",
    "Felix's NLP class lecture slides 6, 9, 13\n",
    "\n",
    "Lecture 30 personal notes\n",
    "\n",
    "Speech and Language Processing Ch 8"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
