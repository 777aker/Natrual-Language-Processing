{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kelley Kelley\n",
    "## Section 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 1:\n",
    "\n",
    "The EM algorithm is used to create alignment probabilites for an IBM Model 2 instead of using the same probability for all alignments like with IBM Model 1\n",
    "\n",
    "a. The model is initialized with either random probabilities or probabilities generated by IBM model 1\n",
    "\n",
    "b. The E step guess the underlying counts, increment alignment counts by a function delta\n",
    "\n",
    "c. The M step then calculates new values based upon the counts from the E step\n",
    "\n",
    "d. Our labeled data doesn’t have alignments, only sentences and their translation, but even if it did have alignments we would still have a lot of missing alignments since a lot of combinations can’t be made"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2:\n",
    "\n",
    "a. The encoder takes the source and turns it into an embedding. At each time step it is a single vector from the input sequence, as a whole it would be a matrix of the vectors for every input sequence. \n",
    "\n",
    "b. The decoder takes the embeddings and outputs a target word. \n",
    "\n",
    "c. Only the current word from the embeddings. The embeddings contain only the previous word so it is kind of only aware of the previous word.\n",
    "\n",
    "d. Use beam search so we can get better inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 3:\n",
    "\n",
    "Transfer learning can be used in order to get lots of data for something you don't have data for. For example, if you didn't have enough data to translate from English to Spanish. You could use a model to create embeddings on the English and Spanish so you could train the encoder and decoder on just one language at a time then combine them in order to form a complete model.\n",
    "\n",
    "One of the limitations it faces is that it is still difficult to varify the accuracy on languages with lower amounts of data. Even if you use transfer learning you still can't completely make up for a sparsity in data. The representations of a language may be different in a lower sourced language then in something like English which has huge amounts of data, and there isn't really a way to tell if you are correctly making up for this without more data to test it on. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 4:\n",
    "\n",
    "<mark style=\"background-color: yellow !important; font-weight:bold\"> b. </mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 5:\n",
    "\n",
    "<mark style=\"background-color: yellow !important; font-weight:bold\"> a. c. </mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Citations:\n",
    "\n",
    "Felix's NLP Lecture Slides 35, 38\n",
    "Personal Lecture 33 Notes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
