{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kelley Kelley\n",
    "## Section 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 1:\n",
    "\n",
    "One way that NLP systems are bias is that it typically will use masculine words over feminine words in American culture when converting text to other languages. Let's say you were using IBM model 2 to translate text from English to Spanish. In English, male words are typically used more and if you don't know the gender of something it is typical to use male pronouns or descriptors. Since masculine words are more probable, it will map masculine words to feminine words as well. In Spanish however, male and female is extremely important and words have different meanings based upon whether it is masculine or feminine. It's an extremely difficult problem to solve because it isn't realistic to curate the data in order to be correct. Going over every example and labeling it would be quite difficult. Also if you try to solve bias depending on how you solve it you will probably introduce a new form of bias. And lastly, since Spanish is structurally different and how it uses masculine and feminine words is simply not seen in English and an IBM 2 model cannot capture this discrepency between semantic meanings and the intricicies of different languages and how the contexts of words changes the meaning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Citations:\n",
    "\n",
    "Felix's NLP Lecture Slides 36"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
