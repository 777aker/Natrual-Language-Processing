{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nltk` and other useful python libraries\n",
    "----------------------\n",
    "Lecture notebook for CSCI 3832, Spring 2020, Lecture 3, 1/17/2020.\n",
    "\n",
    "(See the Getting Started/Python module on Canvas for more info on Jupyter Notebooks and python. We'll be using python 3 in this class.)\n",
    "\n",
    "\n",
    "- [`nltk`](https://www.nltk.org/) is a useful python library that has many NLP tools built in. It's a great tool to use to get the hang of things and to explore NLP. We'll be using it for exploratory examples. You are **not** allowed to use `nltk` for your homework assignments.\n",
    "- [`collections`](https://docs.python.org/3/library/collections.html) is a python module (built-in) that provides fancier (and sometimes more useful) data structures for you to use. `Counter` and `defaultdict` are particularly useful.\n",
    "- [`matplotlib.pyplot`](https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.html) is a useful subset of the `matplotlib` library, which lets us graph things! It has a simpler interface in general than `matplotlib` as a whole.\n",
    "- others, which we'll see later in class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "#nltk.download(\"punkt\")  # you may need to do this if this is your first time running nltk\n",
    "\n",
    "# so that graphs will show up in this notebook (so that we can see them)\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read in a file in the format:\n",
    "# freq type\n",
    "# into a distionary\n",
    "def record_freqs(file):\n",
    "    lex_freqs = {}\n",
    "    with open(file, \"r\") as f:\n",
    "        for line in f:\n",
    "            count, word = line.strip().split()\n",
    "            lex_freqs[word] = int(count)\n",
    "    return lex_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs = record_freqs(\"shakes_freqs.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 23526\n"
     ]
    }
   ],
   "source": [
    "# what is the size of our vocabulary? \n",
    "print(\"vocab size:\", len(freqs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num tokens: 926286\n"
     ]
    }
   ],
   "source": [
    "# what about the number of tokens?\n",
    "\n",
    "# added between lecture 3 & lecture 4\n",
    "num_tokens = sum(freqs.values())\n",
    "print(\"num tokens:\", num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is the size of our vocabulary if we used nltk?\n",
    "f = open(\"shakesdown.txt\", \"r\")\n",
    "content = f.read()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nltk.word_tokenize(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num tokens: 1110213\n",
      "vocab size: 29495\n"
     ]
    }
   ],
   "source": [
    "print(\"num tokens:\", len(tokens))\n",
    "print(\"vocab size:\",len(set(tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the end, we won't want to be dealing with lists of strings for words. Mapping elements of the vocabulary to integers is going to be more efficient. So we'll create a new dictionary that maps words (as strings) to integers. Anything that we need to know about a word (like its part of speech) we'll associate with its integer index. We'll use defaultdict to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index = collections.defaultdict(lambda: len(word2index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# we'll be updating our lexicon on 1/22 here\n",
    "#len(word2index)\n",
    "UNK = word2index[\"<UNK>\"]\n",
    "print(UNK)\n",
    "print(word2index[\"<UNK>\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a lexicon, we'll set an threshold for membership in the vocab. If you're above some frequency in the training data, you're in. Let's set it to 2 for Shakespeare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon = [word2index[word] for word, freq in freqs.items() if freq > threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexicon[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11655"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what does this number tell us?\n",
    "len(word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47\n"
     ]
    }
   ],
   "source": [
    "# what does the number that gets printed out here tell us?\n",
    "word2index[\"horatio\"]\n",
    "\n",
    "# added between lecture 3 & 4: how can we know how many times horatio occurs?\n",
    "print(freqs[\"horatio\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll be updating our lexicon on 1/22 here\n",
    "word2index = collections.defaultdict(lambda: UNK, word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create a reverse dictionary of the one above\n",
    "index2word = { index:word for word, index in word2index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = \"to be or not to be that is the question\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "19\n",
      "45\n",
      "13\n",
      "4\n",
      "19\n",
      "9\n",
      "11\n",
      "1\n",
      "693\n"
     ]
    }
   ],
   "source": [
    "# how do we get the indexes for these words out of our original lexicon?\n",
    "# to be finished weds 1/22!\n",
    "split = test1.split()\n",
    "for word in split:\n",
    "    print(word2index[word])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\n",
      "and\n",
      "i\n",
      "some\n",
      "cleopatra\n",
      "extremes\n"
     ]
    }
   ],
   "source": [
    "# what about converting a list of indices into a list of words?\n",
    "indices = [1, 2, 3, 100, 375, 4443]\n",
    "for ind in indices:\n",
    "    print(index2word[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "i\n",
      "995\n",
      "went\n",
      "4\n",
      "to\n",
      "1\n",
      "the\n",
      "0\n",
      "<UNK>\n",
      "3040\n",
      "today\n"
     ]
    }
   ],
   "source": [
    "# let's look at a trickier test sentence\n",
    "test2 = \"i went to the cinema today\"\n",
    "split = test2.split()\n",
    "for word in split:\n",
    "    print(word2index[word])\n",
    "    print(index2word[word2index[word]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index2word[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
