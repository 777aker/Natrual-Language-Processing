{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CSCI 3832: Lecture 8, investigating classifiers, precision, recall\n",
    "===========\n",
    "1/31/2020, Spring 2020, Muzny\n",
    "\n",
    "Relevant textbook sections: 4.1, 4.3, 4.7\n",
    "\n",
    "Today, we'll be spending our time investigating some classifiers that we've trained for you.\n",
    "\n",
    "All three of these classifiers are Naïve Bayes classifiers. For a given new, unlabeled document, they calculate:\n",
    "\n",
    "$$ P(feature_1, feature_2, feature_3, ..., feature_n | c)P(c)$$\n",
    "\n",
    "Where $c$ is a candidate class. They then select the class that has the highest probability to be the actual label of the new document.\n",
    "\n",
    "\n",
    "Task 1: Which Classifier is Which?\n",
    "-------------------------\n",
    "We have given you 3 Naïve Bayes classifiers. All three of these are binary classifiers that choose between the label '0' or '1' (these are strings).\n",
    "\n",
    "- one of these classifiers is an authorship attributor\n",
    "- one of these classifiers is a language identifier\n",
    "- one of these classifiers is a sentiment analyser\n",
    "\n",
    "Your first job is to conduct experiments to determine two things:\n",
    "1. Which classifier is which?\n",
    "2. What specific classes do you believe that they are choosing between? (what are better labels for each classifier than '0' and '1'?)\n",
    "    1. Note: this is a difficult task. It is of utmost importance that you consider the particular data set that they were trained on. I will tell you that they were trained using some of [nltk's available corpora](http://www.nltk.org/nltk_data/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authorship 0 or 1 is probably is it this author or not or which of two authors\n",
    "Language identifier is probably 0 for not english 1 for english\n",
    "Sentiment analyser is probably negative emotion vs positive emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Write your names here\n",
    "# Names: Kelley Kelley and Jake Swartwout\n",
    "# Feel free to work in groups of 2 - 3/talk to your neighbors\n",
    "\n",
    "# You'll be turning this notebook in at the end of lecture today \n",
    "# as a pdf\n",
    "# File -> Download As -> .html -> open in a browser -> print to pdf\n",
    "# (one submission per group)\n",
    "# Please make a comment on your submission with your name and the name(s)\n",
    "# of your partners as well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load your trained classifiers from pickled files\n",
    "# (we've already trained your classifiers for you)\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt # for graphing\n",
    "#import nltk  # not necessary, but you can uncomment if you want\n",
    "\n",
    "# add more imports here as you would like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function converts a list of words so that they are featurized\n",
    "# for nltk's format for bag-of-words\n",
    "# params:\n",
    "# words - list of words where each element is a single word \n",
    "# return: dict mapping every word to True\n",
    "def word_feats(words):\n",
    "    return dict([(word, True) for word in words])\n",
    "\n",
    "f = open('classifier1.pickle', 'rb')\n",
    "classifier1 = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open('classifier2.pickle', 'rb')\n",
    "classifier2 = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open('classifier3.pickle', 'rb')\n",
    "classifier3 = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "# in a list, if you find that helpful\n",
    "classifiers = [classifier1, classifier2, classifier3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['0', '1'])\n",
      "0.6325082240556184\n",
      "0.3674917759443814\n",
      "0\n",
      "dict_keys(['0', '1'])\n",
      "3.7824855426585115e-08\n",
      "0.9999999621751425\n",
      "1\n",
      "dict_keys(['0', '1'])\n",
      "0.867841315914037\n",
      "0.1321586840859627\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Here's an example of how to run a test sentence through the classifiers\n",
    "# edit at your leisure\n",
    "test = \"this is a test sentence\"\n",
    "# you can either split on whitespace or use nltk's word_tokenize\n",
    "featurized = word_feats(test.split()) \n",
    "for classifier in classifiers:\n",
    "    print(classifier.prob_classify(featurized).samples())  # will tell you what samples are available\n",
    "    print(classifier.prob_classify(featurized).prob('0'))  # get the probability for class '0'\n",
    "    print(classifier.prob_classify(featurized).prob('1'))  # get the probability for class '1'\n",
    "    print(classifier.classify(featurized))  # just get the label that it wants to assign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: put in as many experiments as you'd like here (and feel free to add more cells as needed)\n",
    "# we recommend testing a variety of sentences. You can make these up or get them from sources\n",
    "# on the web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emotions tests\n",
      "----------------\n",
      "Classifier:  1\n",
      "bad negative\n",
      "P(0):  0.5604746317512275\n",
      "P(1):  0.4395253682487722\n",
      "Label:  0\n",
      "----------------\n",
      "Classifier:  2\n",
      "bad negative\n",
      "P(0):  0.00039311220731488405\n",
      "P(1):  0.9996068877926841\n",
      "Label:  1\n",
      "----------------\n",
      "Classifier:  3\n",
      "bad negative\n",
      "P(0):  0.8255161545582594\n",
      "P(1):  0.17448384544174098\n",
      "Label:  0\n",
      "----------------\n",
      "Classifier:  1\n",
      "suck worst sad\n",
      "P(0):  0.8340384311244659\n",
      "P(1):  0.16596156887553373\n",
      "Label:  0\n",
      "----------------\n",
      "Classifier:  2\n",
      "suck worst sad\n",
      "P(0):  0.008065476075267777\n",
      "P(1):  0.9919345239247326\n",
      "Label:  1\n",
      "----------------\n",
      "Classifier:  3\n",
      "suck worst sad\n",
      "P(0):  0.3137165678868946\n",
      "P(1):  0.6862834321131046\n",
      "Label:  1\n",
      "----------------\n",
      "Classifier:  1\n",
      "good amazing\n",
      "P(0):  0.3724219126783736\n",
      "P(1):  0.6275780873216263\n",
      "Label:  1\n",
      "----------------\n",
      "Classifier:  2\n",
      "good amazing\n",
      "P(0):  0.0009919164649184365\n",
      "P(1):  0.9990080835350811\n",
      "Label:  1\n",
      "----------------\n",
      "Classifier:  3\n",
      "good amazing\n",
      "P(0):  0.9207732071190304\n",
      "P(1):  0.07922679288096905\n",
      "Label:  0\n",
      "----------------\n",
      "Classifier:  1\n",
      "beautiful wonderful happy\n",
      "P(0):  0.16510928521561666\n",
      "P(1):  0.8348907147843831\n",
      "Label:  1\n",
      "----------------\n",
      "Classifier:  2\n",
      "beautiful wonderful happy\n",
      "P(0):  0.00030213277771872356\n",
      "P(1):  0.9996978672222794\n",
      "Label:  1\n",
      "----------------\n",
      "Classifier:  3\n",
      "beautiful wonderful happy\n",
      "P(0):  0.9995577589822459\n",
      "P(1):  0.00044224101775415596\n",
      "Label:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"emotions tests\")\n",
    "emotionstests = []\n",
    "emotionstests.append(\"bad negative\")\n",
    "emotionstests.append(\"suck worst sad\")\n",
    "emotionstests.append(\"good amazing\")\n",
    "emotionstests.append(\"beautiful wonderful happy\")\n",
    "i = 1\n",
    "for words in emotionstests:\n",
    "    i = 1\n",
    "    for classifier in classifiers:\n",
    "        print(\"----------------\")\n",
    "        print(\"Classifier: \", i)\n",
    "        i += 1\n",
    "        featurized = word_feats(words.split())\n",
    "        print(words)\n",
    "        print(\"P(0): \", classifier.prob_classify(featurized).prob('0'))\n",
    "        print(\"P(1): \", classifier.prob_classify(featurized).prob('1'))\n",
    "        print(\"Label: \", classifier.classify(featurized))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English tests\n",
      "----------------\n",
      "Classifier:  1\n",
      "This is english\n",
      "P(0):  0.43136194115751164\n",
      "P(1):  0.5686380588424879\n",
      "Label:  1\n",
      "----------------\n",
      "Classifier:  2\n",
      "This is english\n",
      "P(0):  7.873301860098114e-06\n",
      "P(1):  0.9999921266981396\n",
      "Label:  1\n",
      "----------------\n",
      "Classifier:  3\n",
      "This is english\n",
      "P(0):  0.7461045155824994\n",
      "P(1):  0.2538954844175007\n",
      "Label:  0\n",
      "----------------\n",
      "Classifier:  1\n",
      "This is also english\n",
      "P(0):  0.37721130109440043\n",
      "P(1):  0.6227886989055992\n",
      "Label:  1\n",
      "----------------\n",
      "Classifier:  2\n",
      "This is also english\n",
      "P(0):  9.588705428952318e-09\n",
      "P(1):  0.9999999904112946\n",
      "Label:  1\n",
      "----------------\n",
      "Classifier:  3\n",
      "This is also english\n",
      "P(0):  0.9349539620177195\n",
      "P(1):  0.06504603798228063\n",
      "Label:  0\n",
      "----------------\n",
      "Classifier:  1\n",
      "fjdjf djaf fdksjf\n",
      "P(0):  0.5\n",
      "P(1):  0.5\n",
      "Label:  1\n",
      "----------------\n",
      "Classifier:  2\n",
      "fjdjf djaf fdksjf\n",
      "P(0):  0.5256709451575262\n",
      "P(1):  0.47432905484247373\n",
      "Label:  0\n",
      "----------------\n",
      "Classifier:  3\n",
      "fjdjf djaf fdksjf\n",
      "P(0):  0.8372395833333334\n",
      "P(1):  0.16276041666666669\n",
      "Label:  0\n",
      "----------------\n",
      "Classifier:  1\n",
      "&#^$^@&!#*&%&#&$\n",
      "P(0):  0.5\n",
      "P(1):  0.5\n",
      "Label:  1\n",
      "----------------\n",
      "Classifier:  2\n",
      "&#^$^@&!#*&%&#&$\n",
      "P(0):  0.5256709451575262\n",
      "P(1):  0.47432905484247373\n",
      "Label:  0\n",
      "----------------\n",
      "Classifier:  3\n",
      "&#^$^@&!#*&%&#&$\n",
      "P(0):  0.8372395833333334\n",
      "P(1):  0.16276041666666669\n",
      "Label:  0\n",
      "----------------\n",
      "Classifier:  1\n",
      "Just curious & if $50 this; is it english?!?!\n",
      "P(0):  0.633894680447915\n",
      "P(1):  0.36610531955208536\n",
      "Label:  0\n",
      "----------------\n",
      "Classifier:  2\n",
      "Just curious & if $50 this; is it english?!?!\n",
      "P(0):  7.739961003150376e-11\n",
      "P(1):  0.9999999999225946\n",
      "Label:  1\n",
      "----------------\n",
      "Classifier:  3\n",
      "Just curious & if $50 this; is it english?!?!\n",
      "P(0):  0.6819542236234908\n",
      "P(1):  0.318045776376508\n",
      "Label:  0\n",
      "----------------\n",
      "Classifier:  1\n",
      "你哄我\n",
      "P(0):  0.5\n",
      "P(1):  0.5\n",
      "Label:  1\n",
      "----------------\n",
      "Classifier:  2\n",
      "你哄我\n",
      "P(0):  0.5256709451575262\n",
      "P(1):  0.47432905484247373\n",
      "Label:  0\n",
      "----------------\n",
      "Classifier:  3\n",
      "你哄我\n",
      "P(0):  0.8372395833333334\n",
      "P(1):  0.16276041666666669\n",
      "Label:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"English tests\")\n",
    "englishtests = []\n",
    "englishtests.append(\"This is english\")\n",
    "englishtests.append(\"This is also english\")\n",
    "englishtests.append(\"fjdjf djaf fdksjf\")\n",
    "englishtests.append(\"&#^$^@&!#*&%&#&$\")\n",
    "englishtests.append(\"Just curious & if $50 this; is it english?!?!\")\n",
    "englishtests.append(\"你哄我\")\n",
    "for words in englishtests:\n",
    "    i = 1\n",
    "    for classifier in classifiers:\n",
    "        print(\"----------------\")\n",
    "        print(\"Classifier: \", i)\n",
    "        i += 1\n",
    "        featurized = word_feats(words.split())\n",
    "        print(words)\n",
    "        print(\"P(0): \", classifier.prob_classify(featurized).prob('0'))\n",
    "        print(\"P(1): \", classifier.prob_classify(featurized).prob('1'))\n",
    "        print(\"Label: \", classifier.classify(featurized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authorship\n",
      "----------------\n",
      "Classifier:  1\n",
      "Harry Potter\n",
      "1\n",
      "----------------\n",
      "Classifier:  2\n",
      "Harry Potter\n",
      "0\n",
      "----------------\n",
      "Classifier:  3\n",
      "Harry Potter\n",
      "0\n",
      "----------------\n",
      "Classifier:  1\n",
      "Where art thou Romeo\n",
      "1\n",
      "----------------\n",
      "Classifier:  2\n",
      "Where art thou Romeo\n",
      "1\n",
      "----------------\n",
      "Classifier:  3\n",
      "Where art thou Romeo\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(\"Authorship\")\n",
    "authors = []\n",
    "# these are the two authors you always mention so its one of these\n",
    "authors.append(\"Harry Potter\")\n",
    "authors.append(\"Where art thou Romeo\")\n",
    "for words in authors:\n",
    "    i = 1\n",
    "    for classifier in classifiers:\n",
    "        print(\"----------------\")\n",
    "        print(\"Classifier: \", i)\n",
    "        i += 1\n",
    "        featurized = word_feats(words.split())\n",
    "        print(words)\n",
    "        print(\"P(0): \", classifier.prob_classify(featurized).prob('0'))\n",
    "        print(\"P(1): \", classifier.prob_classify(featurized).prob('1'))\n",
    "        print(\"Label: \", classifier.classify(featurized))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Answer the questions outlined at the beginning of this task here (please keep __bold__ formatting in this notebook):\n",
    "\n",
    "1. Which classifier is which?\n",
    "    1. classifier1 is __Emotions. It responded to emotion words.__\n",
    "    1. classifier2 is __English. It was 1 every time I used English while the others varied. It was also 0 when I didn't use English__\n",
    "    1. classifier3 is __Authorship. Process of elimination.__\n",
    "2. What specific classes do you believe that they are choosing between?\n",
    "    1. classifier1's '0' label should be __NEGATIVE (I think it is more specific but not sure, maybe sadness, from Jake's pretty graphs we determined it is negative review where angry and hurtful are especially polarizing toward 0)__ and its '1' label should be __POSITIVE (I think happy specifically, from Jake's graphs good movie reviews)__\n",
    "    1. classifier2's '0' label should be __NOT ENLGISH (I think it is a specific other language because Chinese was very neutral, Niko says its spanish)__ and its '1' label should be __English__\n",
    "    1. classifier3's '0' label should be __NOT SHAKESPEARE (not sure who)__ and its '1' label should be __SHAKESPEARE__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2: Investigating Accuracy, Precision, and Recall\n",
    "---------------------------------------------\n",
    "Textbook: 4.7\n",
    "\n",
    "When we are determining how well a classifier is doing, we can look at overall accuracy:\n",
    "\n",
    "$$ accuracy = \\frac{true_{pos} + true_{neg}}{true_{pos} + false_{pos} + true_{neg} + false_{neg}} $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement this accuracy function, \n",
    "# then test the accuracy of two of the three classifiers from task 1.\n",
    "\n",
    "# Params: gold_labels, a list of labels assigned by hand (\"truth\")\n",
    "# predicted_labels, a corresponding list of labels predicted by the system\n",
    "# return: double accuracy (a number from 0 to 1)\n",
    "def accuracy(gold_labels, predicted_labels):\n",
    "    pass\n",
    "\n",
    "\n",
    "# test the accuracy of two of your classifiers.\n",
    "# Note: this requires knowing what labels your test data should have!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, (if you get this far).\n",
    "\n",
    "Often, however, it is more useful to look at __precision__ and __recall__ to determine how well a classifier is doing. This is especially important if we're dealing with imbalanced classes (one class occurs more frequently than another).\n",
    "\n",
    "$$ precision = \\frac{true_{pos}}{true_{pos} + false_{pos}} $$\n",
    "\n",
    "\n",
    "\n",
    "$$ recall = \\frac{true_{pos}}{true_{pos} + false_{neg}} $$\n",
    "\n",
    "To make this calculation, we'll need to choose which label is associated with \"positive\" and which is associated with \"negative\". For our purposes, we'll choose the label '1' to be our \"positive\" label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer the following questions:\n",
    "\n",
    "1. Suppose you wanted a very precise system, but didn't care about recall. How would you achieve this?\n",
    "    1. __YOUR ANSWER HERE__\n",
    "\n",
    "2. Suppose you wanted a system with the best recall, but didn't care about precision. How would you achieve this?\n",
    "    1. __YOUR ANSWER HERE__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement the precision and recall functions, \n",
    "# then test the precision/recall of two of the three classifiers from task 1.\n",
    "\n",
    "# Params: gold_labels, a list of labels assigned by hand (\"truth\")\n",
    "# predicted_labels, a corresponding list of labels predicted by the system\n",
    "# target_label (default value '1') - the label associated with \"positives\"\n",
    "# return: double precision (a number from 0 to 1)\n",
    "def precision(gold_labels, predicted_labels, target_label = '1'):\n",
    "    pass\n",
    "\n",
    "# Params: gold_labels, a list of labels assigned by hand (\"truth\")\n",
    "# predicted_labels, a corresponding list of labels predicted by the system\n",
    "# target_label (default value '1') - the label associated with \"positives\"\n",
    "# return: double recall (a number from 0 to 1)\n",
    "def recall(gold_labels, predicted_labels, target_label = '1'):\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
